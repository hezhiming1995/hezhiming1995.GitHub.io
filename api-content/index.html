{"posts":[{"title":"数据仓库和数据湖之间到底有什么区别？","content":"最近开始面试，面试中经常被问道“数据仓库和数据湖之间有什么区别和相同之处”。写下此篇博客，用以加深和引导大家更好的理解这两个概念~ 首先，必须要承认一点的就是，IT界真的是一个非常喜欢“创造”的行业，这里的“创造”打上了引号其实是特指整个行业喜欢造新词，刻意的创造一些新词汇/新概念用以凸显（我也不知道是为了凸显高大上，还是为了凸显深奥晦涩），反正很多概念捏揉起来，确实有时间让人望而却步！不过，但凡多花一些时间去理解吸收，也就感叹不过如此~ 1. 什么是数据仓库？ 数据仓库的概念最早由美国学者Bill Inmon在1990年在《数据仓库：管理信息的革命》一书中正式提出。当时，随着计算机技术的普及，企业的数据量呈现爆炸式增长。传统的数据库无法满足企业对数据分析的需求，因此数据仓库应运而生。 1.1 数据仓库的定义主要有以下4个方面： 面向主题： 数据仓库的数据是按照业务主题进行组织的，这样可以方便用户进行分析和查询。 集成： 数据仓库的数据来自多个业务系统，经过整合后存储在一个集中的存储库中。 相对稳定： 数据仓库的数据相对稳定，不像业务系统的数据那样经常发生变化。 反应历史变化： 数据仓库的数据可以反映企业的历史数据，这对于企业进行分析和决策非常重要。 1.2 数据仓库具有以下3个主要特性： 数据一致性： 数据仓库的数据必须是一致的，这对于数据分析的准确性非常重要。 数据完整性： 数据仓库的数据必须是完整的，这对于数据分析的有效性非常重要。 数据安全性： 数据仓库的数据必须是安全的，这对于企业的竞争力非常重要。 其中，不管数据仓库在这几年如何发展，有一点可以肯定的就是，数据仓库的核心目的就是为了：支持管理决策。 2. 什么是数据湖？ 如果说数据仓库是正儿八经的从学校走出来的技术概念，那么数据湖就是完全由企业或各种IT公司提出的。最早可以追溯到2001年，当时 Hadoop 项目的创始人 Doug Cutting 和 Mike Cafarella 提出了 “Data Lake” 的概念。他们认为，随着数据量的不断增长，企业需要一种更灵活的数据存储方式，能够存储各种格式的数据。 2.1 数据湖的定义 数据湖（Data Lake）是一种是一种不断演进中、可扩展的大数据存储、处理、分析的基础设施，用于存储所有结构化、半结构化和非结构化数据，包括来自不同来源、不同格式的数据。数据湖可以按原样存储数据，无需先对数据进行结构化处理，并可以运行不同类型的分析，包括控制面板和可视化、大数据处理、实时分析和机器学习。 2.2 数据湖通常具有以下特性 多格式：数据湖可以存储各种格式的数据，包括结构化、半结构化和非结构化数据。 统一存储：数据湖可以为企业提供一个单一的存储位置，用于存储所有数据。 灵活分析：数据湖可以用于各种分析，包括传统分析、实时分析和机器学习。 随着数据量的不断增长，数据湖的使用越来越普遍。数据湖可以帮助企业更有效便捷地存储和管理数据，并进行更深入的分析。 近年来，随着大数据和人工智能的兴起，数据湖也开始面临新的挑战。大数据通常包含大量非结构化数据，传统的数据仓库难以处理。因此，数据湖开始与大数据技术融合，形成了新的数据湖架构，例如湖仓一体等。 3 数据仓库 VS 数据湖 3.1 数仓和数据湖的相同点 通过以上二者的介绍不难看出，数据仓库和数据湖都具有以下相同点： 两者都是用于存储和管理数据的技术。 两者都可以用于进行数据分析。 两者都需要数据治理和安全措施。 3.2 数仓和数据湖的区别： ","link":"https://hezhiming1995.github.io/post/shu-ju-cang-ku-he-shu-ju-hu-zhi-jian-dao-di-you-shi-me-qu-bie/"},{"title":"PVE下虚拟机和LXC容器心跳检测并重启","content":"PVE下虚拟机和LXC容器会有概率性的宕机（原因不详，可能是硬件问题（温度，资源，驱动），也可能系统问题），下面脚本是用于每隔15分钟检测一次虚拟机是否有心跳，如果没有心跳，就强制重启虚拟机（含LXC容器）。 脚本主体 #!/usr/bin/env bash # 定义一个函数，用于检查并重启虚拟机或LXC容器 function check_and_restart() { type=&quot;${1}&quot; # 虚拟机或LXC容器的类型 vm_id=&quot;${2}&quot; # 虚拟机或LXC容器的ID vm_ip=&quot;${3}&quot; # 虚拟机或LXC容器的IP地址 # 使用ping命令检测虚拟机或LXC容器是否可达 ping -c 1 &quot;${vm_ip}&quot; &gt; /dev/null if [[ $? != 0 ]]; then # 记录当前时间和信息 now=$(timedatectl status | grep 'Local time' | awk -F&quot;Local time: &quot; '{ print $2 }') echo &quot;[${now}] [NO] id = ${vm_id}, ip = ${vm_ip} ($type)&quot; if [[ &quot;${type}&quot; == &quot;vm&quot; ]]; then # 如果是虚拟机，尝试停止和启动虚拟机 /usr/sbin/qm stop &quot;${vm_id}&quot; /usr/sbin/qm start &quot;${vm_id}&quot; elif [[ &quot;${type}&quot; == &quot;lxc&quot; ]]; then # 如果是LXC容器，尝试停止和启动LXC容器 /usr/sbin/pct stop &quot;${vm_id}&quot; /usr/sbin/pct start &quot;${vm_id}&quot; else echo &quot;Unknown VM type for id=${vm_id}&quot; fi fi } # 主函数，用于循环检查虚拟机列表 function main() { vm_list=&quot;${1}&quot; # 虚拟机列表，格式为 type:id:ip for each in ${vm_list}; do type=$(echo &quot;${each}&quot; | awk -F: '{ print $1 }') # 提取虚拟机或LXC容器的类型 vm_id=$(echo &quot;${each}&quot; | awk -F: '{ print $2 }') # 提取虚拟机或LXC容器的ID vm_ip=$(echo &quot;${each}&quot; | awk -F: '{ print $3 }') # 提取虚拟机或LXC容器的IP check_and_restart &quot;${type}&quot; &quot;${vm_id}&quot; &quot;${vm_ip}&quot; # 调用检查并重启函数 done } # 需要检查的虚拟机和LXC容器列表，格式为 type:id:ip，其中type=vm表示虚拟机，type=lxc表示LXC容器 vm_list=&quot; vm:100:192.168.50.61 vm:101:192.168.50.62 lxc:102:192.168.50.63 &quot; # 调用主函数并传入虚拟机和LXC容器列表 main &quot;${vm_list}&quot; 脚本调度 crontab -e ​ */15 * * * * bash /root/check_and_restart.sh &gt;&gt; /root/log.txt systemctl restart cron 注意事项 因为一些虚拟机开机特别慢（以Windows代表），因此时间太短也不行，间隔尽量长一些。否则虚拟机开一半就还得stop再start，陷于永久循环。 ","link":"https://hezhiming1995.github.io/post/pve-xia-xu-ni-ji-he-lxc-rong-qi-xin-tiao-jian-ce-bing-chong-qi/"},{"title":"Shell脚本更新Cloudflare的动态DNS","content":" 起因是在外有访问家庭内网的需求，但是家庭宽带所拥有的公网IP又时常自动刷新，因此需要ddns服务，自己Google学习了一下，通过定时任务解析网卡上的ipv4，去更新Cloudflare上的域名dns解析。 前提条件 我是直接解析在软路由上的，因此我是直接解析软路由上的网卡中wan口的ipv4地址。如果你是内网设备的话，可以尝试通过curl命令请求一些公共服务，以获取自己的IP。下面是部分服务网址： # 国内/国外都可以访问 curl ipinfo.io curl cip.cc curl ident.me curl v4.ident.me # 仅国内可以访问 curl myip.ipip.net # 仅国外可以访问 curl ifconfig.me curl inet-ip.info 需要你有一个域名，并且域名的解析服务器迁移到Cloudflare中； 如果需要你家的宽带有公网的ipv4（ipv6更新待定）； 动态DNS代码 这是动态DNS代码主体（存放于/home/ddns.sh） #!/bin/ash # 设置网络接口控制器，这是VWAN（多拨接口）的第一个 # networkInterface='pppoe-vwan1' # 设置网络接口控制器，这是WAN（单播接口） networkInterface='pppoe-wan' # 获取IPv6地址 # ipv6Address=$(ifconfig &quot;$networkInterface&quot; | grep 'inet6 addr' | grep -v fe80 | awk '{print $3}' | cut -d'/' -f1) # 获取IPv4地址 ipv4Address=$(ifconfig &quot;$networkInterface&quot; | awk '/inet addr/{print substr($2,6)}' | cut -d'/' -f1) lastIPFilePath=/home/last_public_ip.txt logFilePath=/home/public_ip.log # 错误处理 set -e # 验证IP地址格式的函数 validateIP() { local ip=$1 if ! echo &quot;$ip&quot; | grep -E -q '^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$'; then echo &quot;$(date &quot;+%F %T&quot;), Invalid IP address: $ip&quot; &gt;&amp;2 exit 1 fi } # 检查并记录IP地址变化 checkAndRecordIPChange() { local currentIP=$1 local lastIP=$2 if [ &quot;$currentIP&quot; != &quot;$lastIP&quot; ]; then # IP地址发生变化 echo &quot;$currentIP&quot; &gt; &quot;$lastIPFilePath&quot; echo &quot;$(date &quot;+%F %T&quot;), $currentIP&quot; &gt;&gt; &quot;$logFilePath&quot; updateDNSRecord &quot;$currentIP&quot; fi } # 更新DNS记录 updateDNSRecord() { local ip=$1 local apiEndpoint=&quot;https://api.cloudflare.com/client/v4/zones/1bxxxxxxxxxxxxxxxxxxxxxxxx1f/dns_records/31xxxxxxxxxxxxxxxxxxxxxxxxxxb5&quot; local authEmail=&quot;xxxxxxxxxxxxxx@qq.com&quot; local authKey=&quot;1dxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx5d&quot; curl -s -X PUT &quot;$apiEndpoint&quot; \\ -H &quot;X-Auth-Email:$authEmail&quot; \\ -H &quot;X-Auth-Key:$authKey&quot; \\ -H &quot;Content-Type: application/json&quot; \\ --data '{&quot;type&quot;:&quot;A&quot;,&quot;name&quot;:&quot;你的域名网址，例如：home.dashixiong.com&quot;,&quot;content&quot;:&quot;'&quot;${ip}&quot;'&quot;,&quot;ttl&quot;:120,&quot;proxied&quot;:false}' \\ &gt;/dev/null } # 判断当前是否有网卡设备IP，且为标准的IP格式 if [ -z &quot;$ipv4Address&quot; ] || ! validateIP &quot;$ipv4Address&quot;; then echo &quot;$(date &quot;+%F %T&quot;), Bad IPv4 address: $ipv4Address&quot; &gt;&gt; &quot;$logFilePath&quot; exit 1 fi # 如果存在上次记录的IP地址文件 if [ -s &quot;$lastIPFilePath&quot; ]; then lastIP=$(cat &quot;$lastIPFilePath&quot;) checkAndRecordIPChange &quot;$ipv4Address&quot; &quot;$lastIP&quot; else echo &quot;$ipv4Address&quot; &gt; &quot;$lastIPFilePath&quot; echo &quot;$(date &quot;+%F %T&quot;), $ipv4Address&quot; &gt;&gt; &quot;$logFilePath&quot; updateDNSRecord &quot;$ipv4Address&quot; fi Linux cron 定时任务执行 # 每五分钟执行一次，及时发现家庭IP变化所导致的ddns错误 */10 * * * * sh /home/ddns.sh 1&gt;/dev/null 2&gt;&gt;/home/error_ddns.log ","link":"https://hezhiming1995.github.io/post/shell-jiao-ben-geng-xin-cloudflare-de-dong-tai-dns/"},{"title":"ChatGPT注册指南（含攻略）","content":"Bilibili -【VLOG #7 - 火爆全网的 ChatGPT 做你的私人“贾维斯”，教你恋爱，帮你答题！真的智能时代要到来了？】 前言 酝酿了两周，ChatGPT真正的火出了圈了。从十二月五六号开始，互联网像是被ChatGPT点燃了一样，热度持续不下，甚至于我有了一种见证历史的感觉，未来也已不限于想象了！😆 基础介绍 ChatGPT是一个基于人工智能技术的对话系统，它可以与用户进行自然语言的对话交互，帮助用户解决问题、获取信息、进行娱乐等。ChatGPT的技术基于大规模预训练语言模型，通过深度学习算法对海量语料进行学习，从而实现了对自然语言的理解和生成。 ChatGPT可以用于多种应用场景，比如智能客服、智能问答、语音识别、智能翻译、聊天机器人等。ChatGPT可以通过API接口、网站插件等形式接入到不同的应用程序中，为用户提供智能化的对话服务。 总之，ChatGPT是一个基于人工智能技术的对话系统，可以模拟人类的自然语言交流，为用户提供便捷、高效的智能化服务。 注册所需条件 科学上网（这个条件不多介绍，最好是USA的环境更稳定一些） 一个Gmail邮箱账号 一个能够接收验证码的境外手机号码（国内的手机号是不被允许的❌，解决方法下面会提到，第五步） 注册流程 第一步 点击打开 chat.openai.com/auth/login 页面中的 “ Sign up ” 进行相应的账号注册 第二步 在新的页面可选择注册 ChatGPT 账号的方式，只能使用Gmail邮箱的方法才能成功 输入邮箱地址，点击 “ Continue ” ，之后输入密码，进入下一步骤 第三步 在你的Gmail邮箱中找到openAI的邮件，点击验证 第四步 继续在ChatGPT注册页面填入基本信息 第五步 填写ChatGPT手机短信验证码 这里需要使用中国以外的手机号码进行验证，可以在俄罗斯的接码平台sms-activate来完成，该网站支持中文/英文显示界面，支持支付宝。 访问接码平台sms-activate网址： sms-activate.org/ 注册网站账号（偷偷告诉你，选印度号码最便宜） 注册完成后点击右上角 “ 余额充值 ” ，使用支付宝充值 2 美元（平台规则要求最低充值2美元）。 在平台左侧搜索 OpenAI ，然后在国家那里找到印度，点击选择国家后面的加入购物车即可。 然后等一会出现如下界面，这里的手机号拷贝出来(号码中的括号可以去除)，输入到上一步中注册OpenAI的界面上，然后点击 Send code按钮，在下图的界面中等待验证码短信。 恭喜你，如果你顺利的按照上述步骤完成了手机号验证（没出什么意外的话😂），那么此时你将可以使用科技最新宠儿——ChatGPT。 善用ChatGPT (2023年4月28日更新，本答案由ChatGPT生成) 1. 界面展示 2.使用技巧 清晰明了的问题：为了获得更准确和满意的回答，向ChatGPT提问时请尽量明确和具体。避免模糊的问题，最好用简洁的语言描述您的需求。 上下文信息：ChatGPT的回答通常基于其之前的上下文。为了更好地理解您的问题，您可以在提问时提供更多相关背景信息，帮助ChatGPT更好地理解您的需求。 交互式对话：与ChatGPT进行交互式的对话，逐步细化问题和回答，以获取更深入和详细的信息。这样，ChatGPT能够逐步完善回答，提供更多意外之喜。 多样性和创意：通过使用不同的表述方式、提问方式或要求ChatGPT进行创造性思考，您可能会得到更多意想不到的答案。例如，您可以尝试要求ChatGPT用诗歌、故事或幽默方式回答问题。 指定风格和语气：ChatGPT具备模仿不同风格和语气的能力。您可以尝试指定特定的语气，如正式、幽默、友善等，以获得更符合您期望的回答。 审查输出：尽管ChatGPT具有强大的生成能力，但有时也会产生不准确或不恰当的回答。在实际应用中，建议您审查ChatGPT的输出(其实就是高速ChatGPT他的回答是正确还是错误的，并点击feedback)，确保其符合准确性和适用性的标准。 肯定和指导：在与ChatGPT进行交互时，您可以使用肯定的话语来奖励其准确的回答，同时使用指导性的反馈来纠正错误或不完善的回答。这有助于ChatGPT学习并逐步提高回答质量。 适度使用：尽管ChatGPT非常强大，但也有其局限性。避免过度依赖ChatGPT，并在合适的场景下使用。对于涉及敏感信息或专业领域的问题，最好寻求专业人士的帮助。 ","link":"https://hezhiming1995.github.io/post/ChatGPT-Registration-Guide/"},{"title":"三种方式调用SparkSQL","content":"本篇介绍SparkSQL的三种使用方式： Spark Shell CLI Thrift Server 通过spark-shell调用SparkSQL 在启动spark shell时添加mysql-jdbc-driver.jar的依赖，然后import HiveContext，就可以正常使用SparkSQL了。 运行spark shell export SPARK_CLASSPATH=/usr/lib/hadoop/lib/hadoop-lzo.jar:\\ /usr/lib/hive/lib/mysql-jdbc-driver.jar ./bin/spark-shell --master local import org.apache.spark.sql.hive.HiveContext val hiveContext = new HiveContext(sc) hiveContext.sql(&quot;use spark_test&quot;) hiveContext.sql(&quot;show tables&quot;).collect().foreach(println) 输出 [tbldate] [tblstock] [tblstockdetail] 通过CLI调用SparkSQL SparkSQL提供了类似于sql命令行的CLI接口，用户可以直接使用sql语言。 运行spark sql export SPARK_CLASSPATH=/usr/lib/hadoop/lib/hadoop-lzo.jar:\\ /usr/lib/hive/lib/mysql-jdbc-driver.jar ./bin/spark-sql --master local use spark_test; show tables; 输出 [tbldate] [tblstock] [tblstockdetail] 通过ThriftServer调用SparkSQL 开启thrift server export SPARK_CLASSPATH=/usr/lib/hadoop/lib/hadoop-lzo.jar:\\ /usr/lib/hive/lib/mysql-jdbc-driver.jar ./sbin/start-thriftserver.sh \\ --hiveconf hive.server2.thrift.port=10000 \\ --hiveconf hive.server2.thrift.bind.host=localhost \\ --master local 开启beeline client bash /data/git/spark/com.iqiyi/spark-1.1.0-2.5.0-cdh5.2.0-qiyi/bin/beeline !connect jdbc:hive2://localhost:10000 use spark_test; show tables; 输出 +-----------------+ | result | +-----------------+ | tbldate | | tblstock | | tblstockdetail | +-----------------+ ","link":"https://hezhiming1995.github.io/post/san-chong-fang-shi-diao-yong-sparksql/"},{"title":"浅谈 AWS S3","content":"AWS S3 特色 AWS S3（Simple Storage Service）提供了统一的 REST/SOAP 接口，用于统一访问任何数据。可以通过 Amazon Web Services Management Console 直接查询文件或通过配置接入点来进行访问。 对于 S3，数据以对象名（键）和数据（值）的形式存在。 以下是 AWS S3 的一些特点： 无限容量，单个文件最大可达 5TB 高速。每个 Bucket 下每秒可达 3500 PUT/COPY/POST/DELETE 请求或 5500 GET/HEAD 请求 支持版本控制和权限控制能力 具备数据生命周期管理能力 尽管这么多有点如此，但是对象存储也有一个最大的缺点，那就是： 文件整进整出，尝试做任何修改，都需要对其完整下载，修改完再完整上传，因此，如果有版本控制型文件或者是流量敏感情况下，大文件仍然需要频繁修改时，对象存储显然并不适合。 Bucket 要存储数据在 S3 中，首先需要创建一个 Bucket。Bucket 默认是不公开的。 Bucket 具有以下特点： 命名必须全球唯一。每个帐号默认可创建 100 个，最多可申请至 1000 个 创建者拥有权不可转让，也不可将其转移到其他区域 没有对象存储数量限制 Bucket 类似于计算机中的顶层分区，所有的对象必须保存在某一个 Bucket 下面。 1.1 要求 对于 S3 存储桶的命名具体要求，请参考 AWS 的用户指南：存储桶命名规则。 除非有特殊要求，系统环境应该在 Bucket 层进行区分。因为 S3 本质上没有目录的概念，所谓的目录层级只是对 Object 的命名进行特殊表示，以模拟层级结构。 除非业务系统有定向要求，所有的 S3 资源文件访问条件仅限于内部 VPC 访问，禁止公网具有读取权限。 推荐使用 AWS S3 服务的版本控制并结合生命周期配置，从而对特定的 Object 对象（如前缀、大小、标签等）进行历史版本备份。 Object Bucket 中的每个存储数据都是一个对象，由对象名（Key）和数据（Value）组成。 对象的键（Key）可以很长，甚至按照一定前缀格式来指定，从而模拟文件夹的层级结构，例如： s3://cnn-s3-rg-xxx/L1/DIM/TableName/_delta_log/000000000000000002468.json 每个 Object 还包含一些元信息（Meta-data），包括系统指定的文件类型、创建时间、加密算法等，以及用户上传时指定的元信息。一旦 Object 创建后，元信息将无法更改。 我们还可以为 Object 指定最多 10 个实体标签（Etag），标签的键和值的最大长度分别为 128 和 256 个字符。与元信息相比，标签是可以修改和新增的。它们的主要好处是可以与权限控制、生命周期管理和数据分析等功能结合使用。 单个文件上传的最大大小为 5GB。超过该大小需要使用分段上传 API，最大支持 5TB。 2.1 要求 推荐使用默认配置（Standard）作为上传文件的存储类型。如果文件访问频率有变化，可以考虑使用 INTELLIGENT_TIERING（智能分层），但这会产生额外费用。对于不经常访问的数据，推荐使用 STANDARD_IA、ONEZONE_IA。归档数据可以使用 S3 Glacier，最低保存期为 90 天，取出时间为 1分钟至 12小时。S3 Glacier Deep Archive 最低保存期为 180 天，默认取出时间为 12 小时内。 ","link":"https://hezhiming1995.github.io/post/qian-tan-aws-s3/"},{"title":"适用于 AWS Redshift 的 TPC-DS 基准测试","content":" 适用于 AWS Redshift 的 TPC-DS 基准测试 1. 组件环境 * 1.1 什么是TPC-DS？ 1.2 TPC-DS的特性 1.3 TPC-DS 基准测试维度 1.4 TPC-DS 测试表数据详情 2. 测试流程 * 2.1 申请TPC官方Benchmark工具包及文档 2.2 安装TPC-DS工具包 （base on Linux） 2.3 修改建表语句，使其符合 AWS Redshift 标准的DDL 2.4 构建测试数据集 2.5 加载数据到Redshift中 2.6 生成Redshift标准查询流 3. 修改为Redshift标准语法的查询Query 4. 执行Query，统计时间 * 4.1 脚本后台执行Query (python) 5. 结果展示 * 5.1 结论 6 References ( ) 适用于 AWS Redshift 的 TPC-DS 基准测试 简体中文 | English 日期：2021年08月24日 作者：何志明（自编辑&amp;整理） 1. 组件环境 Redshift TPC-DS dc2.large | 3 nodes | 480 GB v3.2.0rc1 1.1 什么是TPC-DS？ 什么是TPC？ TPC (事务处理性能测试委员会)，有两个主要职责：一是制定计算机事务处理能力测试标准，二是监督其执行。其总部位于美国，绝大多数会员都是美、日、西欧的大公司。 目前支持数据库三个方向的Benchmark测试，如下图： TPC-H vs TPC-DS 数据类型： TPC-H: 关系模型第三范式 TPC-DS: 关系模型，星型模型，雪花模型 性能分析： TPC-H：严重依赖于索引，容易被hack TPC-DS：健壮性好，能够比较客观的反映系统的真实性能 1.2 TPC-DS的特性 数据真实，数据量大，且含数据倾斜。 测试案例SQL比较复杂，几乎所有案例都有很高的IO负载和CPU计算需求 测试案例中包含各种业务模型（如分析报告型，迭代式的联机分析型，数据挖掘型等），并且每一个SQL查询测试都是真实的业务需求 TPC-DS v2&amp;v3版本: 性价比计算方式变化 1.3 TPC-DS 基准测试维度 Power测试：是用于评测数据库对单个查询流的处理能力; 99个SQL查询流只执行一次; Throughput测试：是用于测试DBMS 对多个查询流并发查询和操作的处理能力; 数据查询执行两次，每次执行至少20 个以上的并行查询流; 评价指标： Performance (QphDS@SF)：反映每秒的有效查询数据量的性能指标，越大越好； Cost Performance (Price/QphDS@SF)：反映每秒每查询数据量的性价比指标，值越小说明性价比越高； 1.4 TPC-DS 测试表数据详情 TPC-DS v3.2.0 | Scale: 100 (GB) | Total bytes: 102462037125 | Total: 102.47 GB No. Table Name Bytes GiB Row Count Estimate Bytes/Row Dimension/Fact Table Notes 1 store_sales 40671627884 37.88GiB 28795080 1412 Fact Table 通过Store渠道销售商品的订单信息 2 catalog_sales 30872465193 28.75GiB 143997072 214 Fact Table 通过Catalog渠道销售商品的订单信息 3 inventory 8226939134 7.66 GiB 399329984 21 Fact Table 仓储相关信息 4 web_sales 15391511849 14.33GiB 72001240 214 Fact Table 通过Catalog渠道销售商品的订单信息 5 store_returns 3455072075 3.22GiB 287997024 12 Fact Table 通过Store渠道销售商品的退货信息 6 catalog_returns 2264820940 2.11GiB 14404374 157 Fact Table 通过Catalog渠道销售商品的退货信息 7 web_returns 1046331434 0.97GiB 7197670 145 Fact Table 通过web渠道销售商品的退货信息 8 customer_demographics 78739296 1920800 41 Dimension Table 客户基本信用情况 9 customer 267515941 2000000 134 Dimension Table 客户相关信息 10 item 58162791 204000 285 Dimension Table 商品信息 11 customer_address 110154196 1000000 110 Dimension Table 客户地址信息 12 date_dim 10244389 73049 140 Dimension Table 时间（日历）维度信息 13 time_dim 5021380 86400 58 Dimension Table 时间维度信息 14 catalog_page 2837522 20400 139 Dimension Table 商品目录相关信息 15 household_demographics 144453 7200 20 Dimension Table 家庭基本信用信息 16 promotion 123973 1000 124 Dimension Table 商品促销信息 17 store 106418 402 265 Dimension Table 商户信息 18 web_page 197009 2040 97 Dimension Table 商品网页基本信息 19 web_site 6850 24 285 Dimension Table 商品网站基本信息 20 call_center 9326 30 311 Dimension Table 客户服务中心相关信息 21 reason 1904 55 35 Dimension Table 用户退货原因 22 warehouse 1767 15 118 Dimension Table 仓库基本信息 23 ship_mode 1093 20 55 Dimension Table 商品快递信息 24 income_band 308 20 15 Dimension Table 收入信息 2. 测试流程 2.1 申请TPC官方Benchmark工具包及文档 官网下载地址：TPC Download Current Specs/Source 注意事项：申请测试包需要切换到外网环境，否则无法弹出官网的人机验证，如下图 Figure 1； 官方介绍文档不用切换网络环境，可以下载，内容涉及工具包的更新log，查询SQL反馈的实际业务目的，ER图等等...... 如果没有外网条件或嫌麻烦，可以下载在此v3.2.0rc1版本TPC-DS工具包：阿里云盘分享 2.2 安装TPC-DS工具包 （base on Linux） 要安装依赖； yum -y install gcc gcc-c++ libstdc++-devel bison byacc flex 解压缩zip工具包; unzip tpc-ds-3.2.0rc1-tool.zip 进入tools目录，执行Makefile编译（无报错，表示执行成功，如 下图Figure 2）； 2.3 修改建表语句，使其符合 AWS Redshift 标准的DDL tools目录下的 tpcds.sql 文件即TPC提供的标准建表语句，但其并不适用于Redshift； 此步骤可参考AWS官方实践的Benchmark所提供DDL文件(GitHub)； —— Cloud-DWB-Derived-from-TPCDS 官方提供的DDL.sql文件已对于每张测试表都调整好了分布键和排序键，能较好的发挥性能； 但其基于的是TPC-DS v2.13，仍需要修改DDL.sql部分字段； 可参考这里我提供的基于AWS 官方DDL.sql修改后的建表文件； —— TPCDS_for_redshift_DDL 2.4 构建测试数据集 创建测试数据集文件存放的目录； mkdir data100g 在tools目录下，构建测试数据集； ./dsdgen -SCALE 100 -DIR ./data100g -SUFFIX .csv -TERMINATE N ./dsdgen命令参数说明； [admin@cdp-wh-163 ~]$ ./dsdgen -help dsdgen Population Generator (Version 3.2.0) Copyright Transaction Processing Performance Council (TPC) 2001 - 2021 USAGE: dsdgen [options] Note: When defined in a parameter file (using -p), parmeters should use the form below. Each option can also be set from the command line, using a form of '-param [optional argument]' Unique anchored substrings of options are also recognized, and case is ignored, so '-sc' is equivalent to '-SCALE' General Options =============== ABREVIATION = &lt;s&gt; -- build table with abreviation &lt;s&gt; DIR = &lt;s&gt; -- generate tables in directory &lt;s&gt; HELP = &lt;n&gt; -- display this message PARAMS = &lt;s&gt; -- read parameters from file &lt;s&gt; QUIET = [Y|N] -- disable all output to stdout/stderr SCALE = &lt;n&gt; -- volume of data to generate in GB TABLE = &lt;s&gt; -- build only table &lt;s&gt; UPDATE = &lt;n&gt; -- generate update data set &lt;n&gt; VERBOSE = [Y|N] -- enable verbose output PARALLEL = &lt;n&gt; -- build data in &lt;n&gt; separate chunks CHILD = &lt;n&gt; -- generate &lt;n&gt;th chunk of the parallelized data RELEASE = [Y|N] -- display the release information _FILTER = [Y|N] -- output data to stdout VALIDATE = [Y|N] -- produce rows for data validation Advanced Options =============== DELIMITER = &lt;s&gt; -- use &lt;s&gt; as output field separator DISTRIBUTIONS = &lt;s&gt; -- read distributions from file &lt;s&gt; FORCE = [Y|N] -- over-write data files without prompting SUFFIX = &lt;s&gt; -- use &lt;s&gt; as output file suffix TERMINATE = [Y|N] -- end each record with a field delimiter VCOUNT = &lt;n&gt; -- set number of validation rows to be produced VSUFFIX = &lt;s&gt; -- set file suffix for data validation RNGSEED = &lt;n&gt; -- set RNG seed 参数 说明 示例 -scale &lt;n&gt; 生成多少的数据集（GB） -scale 100，生成100GB -DIR &lt;s&gt; 生成的数据集存放于&lt;s&gt;目录下 -DIR ./data100g | 在当前目录下的data100g下生成数据集 -SUFFIX &lt;s&gt; 使用&lt;s&gt;作为输出文件后缀 -SUFFIX .csv | 以.csv为每份数据集的文件名后缀 -TERMINATE &lt;Y|N&gt; 每行最后是否加字段分隔符 N或者Y | N：每行最后不加字段分隔符。Y：每行最后添加字段分隔符。比如分隔符|。 -TABLE &lt;s&gt; 仅生成测试表 &lt;s&gt;的数据 -TABLE call_center -PARALLEL 一共分成几个chunk。一条shell只能生成一个chunk。因此设置了几个，就要执行几次。 -PARALLEL 5 -CHILD 当前shell命令用于生成第几个chunk。 -PARALLEL 5 -CHILD 1 -PARALLEL -CHILD适用于生成超大型的数据（&gt;=1TB），以此合理利用机器性能，从而节省数据准备时间。 因为./dsdgen是单线程程序，所以可以在后台启动多个./dsdgen多进程并发执行数据的生成。 2.5 加载数据到Redshift中 请注意：这里我采用的是先将数据上传至S3，再从S3将数据COPY到Redshift中，有更优的方式请忽略此步骤； 上传测试数据集至S3; aws s3 cp ../data100g/ s3://&lt;PATH&gt;/TPC-DS_v3.2.0/scale-100GB/ --recursive ## 推荐使用nohup后台执行上传操作，操作日志保留在当前目录下的nohup.out文件中 nohup aws s3 cp ../data100g/ s3://&lt;PATH&gt;/TPC-DS_v3.2.0/scale-100GB/ --recursive &amp; 加载S3数据到Redshift中，这里我采用了Python脚本的方式进行COPY； #!/usr/bin/python # -*- coding: UTF-8 -*- import psycopg2 conn = psycopg2.connect(dbname='x', host='x', port='x', user='x', password='x') cursor=conn.cursor() tableName_TPCDS = ('dbgen_version','customer_address','customer_demographics','date_dim','warehouse','ship_mode','time_dim','reason','income_band','item','store','call_center','customer','web_site', 'store_returns','household_demographics','web_page','promotion','catalog_page','inventory','catalog_returns','web_returns','web_sales','catalog_sales','store_sales') for item in tableName_TPCDS: query = &quot;copy &lt;Redshift SCHEMA&gt;.&quot;+item+&quot; from 's3://&lt;S3 PATH of TPC-DS DATA&gt;&quot;+item+&quot;.csv' iam_role 'arn:aws-cn:iam::&lt;XXX&gt;' ACCEPTINVCHARS EMPTYASNULL BLANKSASNULL MAXERROR 3&quot; print(query) cursor.execute(query) cursor.close() conn.commit() conn.close() 请注意：从S3执行COPY命令上载大型文件时，可能会提示 internal Error disk full a. (AWS官方解释:复制大型文件报磁盘已满的错误) b. Solution：24张表的数据分表COPY；压缩后进行COPY；大型事实表分块执行COPY； 校验数据是否有全部导入到Redshift中（匹配Redshift 对应Table中的行数） select count(*) from TABLE_NAME; 2.6 生成Redshift标准查询流 在tools目录下，通过dsdgen命令生成对应的SQL语法及对应数据量级的测试数据 ./dsdgen命令主要参数说明； 参数 说明 示例 -scale &lt;n&gt; 假设数据库的大小为 &lt;n&gt;（GB） -scale 100，生成100GB -DIALECT &lt;s&gt; 生成定义为SQL方言 &lt;s&gt;的查询语句 -DIALECT oracle | 生成oracle语法标准的查询SQL -OUTPUT_DIR &lt;s&gt; 将查询流写入目录 &lt;s&gt; -OUTPUT_DIR ./queries | 在当前目录下的queries目录中生成查询SQL文件 -INPUT &lt;s&gt; 从 &lt;s&gt;文件中读取查询SQL的模板名 -input ../query_templates/templates.lst | 从query_templates目录下的templates.lst文件中读取所有的queryX.tpl查询SQL模板 -DIRECTORY &lt;s&gt; 在 &lt;s&gt; 中查询模板 -DIRECTORY ../query_templates/ | 在query_templates目录中寻找查询SQL模板 -TEMPLATE &lt;s&gt; 仅从模板 &lt;s&gt;生成查询SQL模板 -TEMPLATE &quot;query63.tpl&quot; | 仅根据query63.tpl模板生成查询SQL -FILTER &lt;Y|N&gt; 将生成的查询SQL写入标准输出 (stdout) -FILTER Y &gt;../queries/query63.sql 由上参数说明可知，有两种生成查询SQL的方式：1.单独生成每一个查询SQL； 2.从文件中批量读取模板生成一个SQL但包含多个查询 以下是我编写的两个脚本用于生成打乱顺序的组合查询SQL，和批量生成独立99个的SQL； 在tools目录下，将99个测试Query打乱顺序生成到一个SQL文件中，重复生成10组： #!/usr/bin/python # -*- coding: UTF-8 -*- import random import os print(&quot;Generate TPC-DS 99 Query SQL&quot;) # 生成一个数组x，存储1-99数字 x = [i for i in range(1,100)] for i in range(1,11): # 随机打乱x数组，打乱10次 random.shuffle(x) # 拼接dsqgen可执行的Query SQL集合文件名 templates=&quot;templates_&quot;+str(i)+&quot;.lst&quot; # 打开对应目录下的Query SQL集合文件 tempFile = open(&quot;../query_templates/&quot;+templates,'a') # 随机插入打乱顺序的Query SQL名到集合文件中 for j in range(1,100): tpl = &quot;query&quot;+str(x[j-1])+&quot;.tpl&quot; tempFile.write(tpl+&quot;\\n&quot;) tempFile.close() # 拼接10个总query SQL文件名 totalQueryName = &quot;totalQuery_&quot;+str(i)+&quot;.sql&quot; # 拼接dsqgen命名，用于生成10个总的query SQL cmd = &quot;./dsqgen -input ../query_templates/&quot;+templates+&quot; -directory ../query_templates -dialect netezza -scale 100 -FILTER Y &gt; ../data/queries/Query_100GB/totalQueries/&quot;+totalQueryName os.system(cmd) 在tools目录下，分别独立生成99个测试Query的SQL文件： #!/usr/bin/python # -*- coding: UTF-8 -*- import os print(&quot;generate query sql&quot;) # 遍历1-99 for i in range(1,100): tpl = &quot;query&quot;+str(i)+&quot;.tpl&quot; qsql = &quot;query&quot; +str(i) +&quot;.sql&quot; # 拼接命令 cmd = &quot;./dsqgen -DIRECTORY ../query_templates/ &quot;+&quot;-TEMPLATE &quot;+tpl+&quot; -DIALECT netezza -scale 100 -FILTER Y &gt; &quot;+&quot;../data/Query_100GB/queries/&quot;+qsql #print(cmd) #执行命令 os.system(cmd) 3. 修改为Redshift标准语法的查询Query Tips. 在生成或执行Query的SQL时，会有很多问题。以下是我汇总的所有问题的解决方案。通过修改对应的query模板，使其最终生成Redshift语法的sql。 执行命令时提示：ERROR: Substitution '_END' is used before being initialized… 在工具包的query_templates目录下，找到你所采用的SQL “方言”（Dialect ）文件，官方仅支持ansi，db2，Netezza，oracle，sqlserver这五个SQL Dialect，在您使用的SQL Dialect文件末尾添加以下两列； define _BEGIN = &quot;-- start query &quot; + [_QUERY] + &quot; in stream &quot; + [_STREAM] + &quot; using template &quot; + [_TEMPLATE]; define _END = &quot;&quot;; AWS Redshift不支持ROLLUP &amp; GROUPING函数 —— queries 5, 14, 18, 22, 27, 35, 36, 67, 70, 77, 80, and 86需要变形，可以从../query_variants目录下复制官方提供的已变形文件； AWS Redshift中对Date类的字段加减操作 (e.g., + 14 days) 不适配 —— queries 5, 12, 16, 20, 21, 32, 37, 40, 77, 80, 82, 92, 94, 95 and 98需要变形，去除 days关键字 AWS Redshift 不支持SUBSTR函数 —— queries 8, 15, 19, 23, 45, 62, 79, 85 and 99需要变形，修改为SUBSTRING()函数 基础语法错误 —— query77a.tpl 76行错误，缺少’,’分割符，135行错误，缺少’as’连接符，进行修改 4. 执行Query，统计时间 4.1 脚本后台执行Query (python) 由于篇幅问题，这里只粘贴执行组合query的SQL文件的测试脚本。考量范围有： 开启Redshift Result Cache与不开启对于查询性能的影响； 执行五次，统计取平均值； 设置默认的Redshift Schema，以便不用修改每一个查询sql； 每一次执行保留运行数据到pandas.DataFrame数据结构中，统计结果输出到cvs文件； #!/usr/bin/python # -*- coding: UTF-8 -*- import psycopg2 import pandas as pd import datetime #### /home/admin/TPC-DS_Tools_v3.2.0/data/queries/Query_100GB/totalQueries # Get Redshift Connection conn = psycopg2.connect(dbname='****', host='****', port='****', user='****', password='****') # Create an empty pandas.DataFrame to store excute results df_TPCDS_Result = pd.DataFrame(columns=['SQLName','No#Runs','resultCache','resultPath','startTime','endTime','costTime']) for i in range(1,11): if (i&gt;=6): resultCache = 'on' else: resultCache ='off' for j in range(1,11): # Configure the current querySQL x path queryName = &quot;totalQuery_&quot; +str(j) +&quot;.sql&quot; queryPath = &quot;./totalQueries/&quot; +queryName # open querySQL x queryFile = open(queryPath,'r') querySQL = queryFile.read() # Create Cursor object cursor=conn.cursor() startTime = datetime.datetime.now() # Setting inapplicable result cache cursor.execute(&quot;set enable_result_cache_for_session to %s;&quot; %resultCache) # Setting default schema cursor.execute(&quot;set search_path to &lt;Redshift SCHEMA&gt;;&quot;) # excute querySQL x cursor.execute(querySQL) # Get excute querySQL x result data = cursor.fetchall() endTime = datetime.datetime.now() costSeconds = (endTime - startTime).seconds costMicroSeconds = (endTime - startTime).microseconds costTime = round(costSeconds+costMicroSeconds/1000000,2) print(&quot;The Total Time is: %s seconds&quot; %costTime) queryFile.close() # Query result visualization resultData = pd.DataFrame(data) resultFilePath = &quot;./totalResult/totalQuery&quot;+str(j)+&quot;_No#Runs&quot;+str(i)+&quot;.csv&quot; resultData.to_csv(resultFilePath,header=None,index=None) # print(resultData) conn.commit() cursor.close() newList = {'SQLName':queryName,'No#Runs':i,'resultCache':resultCache,'resultPath':resultFilePath,'startTime':startTime,'endTime':endTime,'costTime':costTime} df_TPCDS_Result = df_TPCDS_Result.append(newList,ignore_index=True) print(df_TPCDS_Result) overallResultFilePath = &quot;./totalResult/overallTotalResultView.csv&quot; df_TPCDS_Result.to_csv(overallResultFilePath) conn.close() 5. 结果展示 原图太大了，这里我放到云盘了(阿里云盘) 下图建议右击在新标签页打开，或保存到本地查看。 5.1 结论 TPC-DS v3.2.0 100GB数据量级 开启Result Cache和不开启Result Cache的查询速率上，相差大约在1~16560倍之间； 绝大多数情况，无论是开启或不开启Result Cache，第一次执行查询所耗时间远高于之后执行相同查询的所耗时间，相差大约在1~9446倍之间； 不开启Result Cache，99个Query的平均查询时间相加共计：2979s (With Frist Run) 不开启Result Cache，99个Query的平均查询时间相加共计：1645s (With Frist Run) 开启Result Cache，99个Query的平均查询时间相加共计：372s (Without Frist Run) 开启Result Cache，99个Query的平均查询时间相加共计：47s (Without Frist Run) 不开启Result Cache，第一次运行Query的查询时间相加为：8316s (02:18:06) 开启Result Cache，第一次运行Query的查询时间相加为：1672s (00:27:52) 6 References [1]: AWS官方测试结论 [2]: AWS官方测试代码 [3]: TPC-DS性能测试及使用方法 [4]: 如何进行TPC-DS测试 [5]: 聊聊TPC那些事儿 [6]: 阿里云：基于标准测试集的测试说明 ","link":"https://hezhiming1995.github.io/post/gua-yong-yu-aws-redshift-de-tpc-ds-ji-zhun-ce-shi/"},{"title":"为什么要进行数据仓库的分层设计呢？","content":"引言 数据仓库是企业中用于集成、存储和分析数据的关键系统。它允许组织从不同来源收集数据，并为决策提供有价值的洞察。然而，随着数据量和复杂性的增加，单一层次的数据仓库已经无法满足企业的需求。因此，进行数据仓库的分层设计变得尤为重要。 1. 为什么需要分层设计？ 数据仓库的分层设计是一种组织数据的方法，将数据按照不同的层次进行划分和管理。这样做有以下几个重要原因： 性能优化：数据仓库中存储的数据通常涵盖多个主题，且数据量庞大。通过分层设计，可以将数据划分为不同的层次，提高查询性能和响应时间。 数据管理：分层设计将数据按照业务功能进行分类，更易于管理、维护和更新。同时，不同层次的数据可以拥有不同的生命周期和备份策略。 数据安全：分层设计可以在不同层次应用不同的安全措施，保护敏感数据免受未授权访问。 2. 分层设计的核心层次 一个典型的数据仓库分层设计通常包含以下几个核心层次： **原始数据层(ODS)：**也称为“稀疏”层，存储来自不同数据源的未经加工的原始数据。这些数据通常以其原始格式进行存储，保留了最全面的细节。 **数据明细层(DWD)：**也称为“加工”层，对原始数据进行清洗、转换和整合。在该层，数据被结构化、去重，并进行数据质量检查。 **企业数据层(DWS)：**也称为“集成”层，将数据处理层的数据与其他企业数据进行整合。该层的数据是面向企业级的数据视图，支持跨部门和跨功能的分析和决策。 **业务智能层(ADS)：**也称为“报表”层，该层提供了直观、易懂的报表和可视化工具，帮助用户进行数据分析和业务洞察。 3. 分层设计的实施方法 **数据抽取与加载（ETL）：**在每个层次中，ETL过程负责将数据从一个层次移动到另一个层次。ETL流程包括数据抽取、数据转换和数据加载。 **数据安全性：**根据数据敏感性和访问权限，合理规划安全策略，确保数据在不同层次的安全性和隔离性。 **性能优化：**在设计数据模型和数据库时，考虑数据量、查询复杂度等因素，以确保查询性能得到优化。 结论 数据仓库分层设计是有效管理大数据量、提高性能和数据质量的关键方法。通过在不同层次组织数据，企业能够更好地理解和利用数据，做出更明智的决策，提升竞争力。因此，在建设数据仓库时，深入了解分层设计的重要性，并合理应用分层设计，对于企业的数据战略是至关重要的。 ","link":"https://hezhiming1995.github.io/post/wei-shi-me-yao-jin-xing-shu-ju-cang-ku-de-fen-ceng-she-ji-ni/"},{"title":"What is DX (Digital Transformation) ? 什么是数字化转型","content":" 1. 什么是数字化转型？（WHAT ？） 2. 为什么会有数字化转型？（Why？） 3. 我们会在哪里运用到数字化，哪些公司需要数字化转型？（Where?） 4. 数字化转型的关键（KEY） 5. 数字化转型的相关技术 6. 数字化转型的战略 参考文章与文献 前言： 什么是数字化转型？这可能是刚刚毕业，面向传统行业或非互联网公司求职的同学（包括我）所面对的一个云雾缠绕的概念吧。其次为什么互联网公司不谈数字化转型呢？ 求职过程中，一直有碰到数字化转型这个名词，其实我对于数字化转型最最浅显的认知便是，尽可能多的将企业的业务安排或者引导到PC上进行查看，操作，处理的一个过程。举个例子： Ps. 以前的航空公司对于订机票，订座位等一些服务，都是通过电话，旅行社然后加上银行汇款完成的；最近十年，几乎所有的航空公司都支持了在线预定等一系列服务。 再举一个例子： Ps. 以前存钱，转账，查账，贷款，买基金买股票等一些金钱操作，基本上都是要去银行大厅才能完成的（当然黑卡VIP也许可以通过电话或者Email进行操作，这不影响你我还是小老百姓😂）但是现在的社会，也是几乎所有的银行都推出了自己的APP，正在努力地将尽可能多的业务转移至Internet进行服务和维护等。 这就是一开始我对于数字化转型的最基础认知！下面我们就以3W1H方法进行深入的理解：企业的数字化转型，倒是如何定义，理解和实现的呢？ 1. 什么是数字化转型？（WHAT ？） 我们先查看一下一些官媒对其的定义吧： 百度百科： 数字化转型（Digital transformation）是建立在数字化转换（Digitization）、数字化升级（Digitalization）基础上， 进一步触及公司核心业务，以新建一种商业模式为目标的高层次转型。数字化转型Digital transformation是开发数字化技术及支持能力以新建一个富有活力的数字化商业模式（见图）。 数字化转型表明，只有企业对其业务进行系统性、彻底的（或重大和完全的）重新定义——而不仅仅是IT，而是对组织活动、流程、业务模式和员工能力的方方面面进行重新定义的时候，成功才会得以实现。 MBA智库百科 数字化转型是指通过利用现代技术和通信手段，改变企业为客户创造价值的方式。如今，数字技术正被融入到产品，服务与流程当中，用以转变客户的业务成果及商业与公共服务的交付方式。这就是数字化转型。这通常需要客户的参与，但也涉及核心业务流程、员工，以及与供应商及合作伙伴的交流方式的变革。 根据高德纳（Gartner）的IT Glossary给出的解释：Digitization反映的是“信息的数字化”，指的是从模拟形态到数字形态的转换过程（the process of changing from analog to digital form），例如从模拟电视到数字电视、从胶卷相机到数字相机、从物理打字机到word软件，其变革的本质都是将信息以“0-1”的二进制数字化形式进行读写、存储和传递。相比而言，Digitalization强调的是**“流程的数字化”，运用数字技术改造商业模式、产生新的收益和价值创造机会，例如企业资源计划（ERP）系统、客户关系管理（CRM）系统、供应链管理（SCM）系统等都是将工作流程进行了数字化，从而倍增了工作协同效率、资源利用效率**，为企业创造了信息化价值。然而，高德纳给数字化转型Digital transformation”下的定义是开发数字化技术及支持能力以新建一个富有活力的数字化商业模式。因此，数字化转型完全超越了信息的数字化或工作流程的数字化，着力于实现“业务的数字化”，使公司在一个新型的数字化商业环境中发展出新的业务（商业模式）和新的核心竞争力。 从以上定义不难看出，数字化转型本质上是一种通过技术实现的商业模式的转型（重新定义）。“通过技术实现转型”的做法可以追溯到几十年（互联网）、数百年（印刷机）、甚至几千年（车轮）前。 而对于一家企业来说，想要彻底实现数字化转型，更应该充分且彻底的对于公司的业务、资源、流程、管理甚至于员工能力进行重新定义并数字化！ 2. 为什么会有数字化转型？（Why？） 关于这一点，其实应该回归这篇文章的前言，为什么数字化转型只存在于传统或非互联网的企业呢？很简单，因为一个原因就是：因为互联网企业给予人们生活太多的便利和观念改变了，以国内为例：淘宝带来了宅家购物，腾讯带来了线上交友，之后泛泛类皆是以一个“革命者”的身份革了多少传统传统工业的“命根子”，在线教育，在线买菜，在线娱乐，在线理财... ... 所以，传统老牌企业不转型？等着被淘汰吗？ 中学政治老师说过这样子一句话，记忆犹新！ 所有革命成功后，都是保守派了。因为时代在变，社会在变，思想当然也随之而变！因此，对于传统行业，若想源源不绝，只有改革，只有转型这一条路可行！ 使用官方的套话来说，那就是： 在当今云计算，大数据，IoT及网络通讯急速发展的今天，上至国家电网、中石化、中石油、上汽等各个行业的头部企业下至城镇小卖部，餐饮小商贩，生活娱乐场所等个体经营者，只有借助数字化转型，才能保持或增强行业持久竞争力。 3. 我们会在哪里运用到数字化，哪些公司需要数字化转型？（Where?） 其实这个问题，本身就是一个很庞大，且不可量化的问题。生活或工作哪里可以智能化，哪里就可以数字化！ 这里有通用，也有不通用的地方。 先以通用来说： 在定义数字化的时候，我们已经说明了业务、资源、流程、管理甚至于员工能力都可以进行数字化重新定义。这是一种符合当下社会发展和需要的一种模式！ 具体而言便是企业公司的系统集成：1.供应链 2.渠道 3.研发 4.管理 5.市场 6.物流 7.营销 8.仓储。这八个方向便是企业的通用转型方向。 再以非通用来说： 其实对于此，我无法给予一个准确且有效的转型策略。当然对于企业来说，非通用型最为基本可行的就是核心业务的转型战略。 4. 数字化转型的关键（KEY） 数字化转型无疑是建立在技术基础之上的。然而只有采取正确的方法，才能成功实现数字化转型。其中最为重要的是商业与公共服务如何才能最好地利用这些技术。在高度互联的数字化世界，为人类创造的价值源于连接性。要成功实现数字化转型，将人置于千万事务的中心是至关重要的。 以人为本的创新是一种方法，它借助数字技术赋力于人，从而创造商业与社会价值。它将人的创造力、由信息衍生的智慧与结合万物和流程的连接性这三大关键价值驱动因素汇集起来。每种价值都源自于三个维度，即人、信息与基础架构。最重要的是，数字化转型必须为人提供价值。通过采用数字技术，使人们能够过上丰富多彩的生活。 5. 数字化转型的相关技术 云平台：基于硬件的服务，提供计算、网络和存储能力。 移动化：在现代移动通信技术、移动互联网技术构成的综合通信平台基础上，通过应用、服务及网络三个层面，实现管理和服务的移动化、电子化和网络化，向社会提供高效优质、规范透明、适时可得、电子互动的全方位管理与服务。 物联网：通过智能感知、识别技术与普适计算、泛在网络的融合应用，实现智能化识别和管理。 人工智能：通过普通电脑实现的智能化。 网络分析：依据网络拓扑关系（结点与弧段拓. 扑、弧段的连通性），通过考察网络元素的空间及. 属性数据，以数学理论模型为基础，对网络的性能特征进行多方面分析。 互联网安全：使网络系统的硬件、软件及其系统中的数据受到保护 云计算：通过网络以按需、易扩展的方式获得所需的服务。 SDCI(软件定义互联基础架构):增强数据中心虚拟化的收益，提高资源灵活性和利用率。 6. 数字化转型的战略 我们认为，传统企业进行数字化转型的有效途径是“生态协同创新”战略。该战略有三个核心理念：创新生态化、生态协同化、协同创新化。 首先，创新生态化意味着要放弃中央研究院式的封闭式创新，构建开放式创新生态系统。传统的大型企业应设立负责技术研发与研发生态建设的“首席技术官”，统筹建立开放式创新生态系统，才能借助数字化技术转型成为本行业内全球技术创新的引领者。 其次，生态协同化意味着要借助大数据智能技术对生态系统进行主动的干涉和管理，实现生态系统的**“量化运营”。进入数字经济时代，万物互联与大数据智能技术的出现使得生态的大规模协同的成本**大为降低、效率大为提高。因此，企业应在集团层面设立“首席数据官”，借助大数据智能技术将研发类生态伙伴、供应链生态伙伴、销售类生态伙伴、人才类生态伙伴、投融资生态伙伴的能力进行有机协同，统筹整个集团体系内生态系统的“量化运营”，为实现生态“大协同”下的快速进化提供数字化养料。 最后，协同创新化是实现数字经济时代“技术创新+商业模式创新”双轮驱动的核心引擎。过去，各个公司的销售部门与产品研发部若能在技术创新方面有所协同就已属难得，只有很少的几个公司能做到“技术创新+商业模式创新”的协同创新化。如果能够将销售端最为敏感的“商业模式创新”与研发端最为敏感的“技术创新”进行强耦合，设立“首席创新官”岗位以有意识地引导“技术创新+商业模式创新”双轮驱动，形成协同创新化的新局面，则必能有力牵引一个传统企业实现数字化转型。 总而言之，传统企业在数字化转型过程中，应考虑设立首席技术官、首席数据官、首席创新官“铁三角”，协力推动企业的生态协同创新战略，实现创新生态化、生态协同化、协同创新化，才能实现数字经济时代“技术创新+商业模式创新”的双轮驱动增长。 参考文章与文献 MBA智库百科：数字化转型 Mckinsey，《中国的数字化转型》 Red Hat，什么是数字化转型？ 百度百科：数字化转型 leafer，企业数字化转型需要经历的五个阶段（推荐！！！） ","link":"https://hezhiming1995.github.io/post/what-is-dx-digital-transformation-shi-me-shi-shu-zi-hua-zhuan-xing/"},{"title":"软件工程的认知与理解","content":" 1. 什么是软件工程？（What？） 2. 为什么会有软件工程？（Why？） 3. 我们会在哪里运用到软件工程？（Where?） 4. 那我们应该怎么运用软件工程呢？（How？） 5. 最后 参考文章与文献： 前言： 在理解任何事物前，希望我可以养成一个对于这个事物提出3W1H（What？Why？Where？How？）的先行分析。再去高谈阔论云云~ 我大学本科专业就是软件工程，当然也是上过《软件系统架构》这门课程，当时对于这门理论不能再理论的课程着实起不了半点儿兴趣（没有深度的理解吸收）。但是这两个月的求职阶段，抛开技术而言，大企业对于我本科阶段最看重的就是软件工程这样一种结构化的思维。 1. 什么是软件工程？（What？） 软件工程一直以来都缺乏一个统一的定义，很多学者、组织机构都分别给出了自己认可的定义，这里我粘贴一下百度百科以及相关文章的定义： Barry Boehm：运用现代科学技术知识来设计并构造计算机程序及为开发、运行和维护这些程序所必需的相关文件资料。 IEEE：在软件工程术语汇编中的定义：软件工程是：1.将系统化的、严格约束的、可量化的方法应用于软件的开发、运行和维护，即将工程化应用于软件；2.在1中所述方法的研究。 比较认可的一种定义认为：软件工程是研究和应用如何以系统性的、规范化的、可定量的工程化方法去开发和维护软件，以及如何把经过时间考验而证明正确的管理技术和当前能够得到的最好的技术方法结合起来。 以上定义可以看出，其实软件工程离不了软件的开发、运作以及后期维护三大环节。在这三个环节中，需要具备的就是一种工程化的思维方式去系统性的、规范化的、可量化的进行软件的全周期！其中，不仅仅限制于开发阶段，需要注意到软件整个项目中：周期有限、技术不断更新、需求不断变化、成本也是有限的这些环节。 2. 为什么会有软件工程？（Why？） 一般我认为3W中Why是最核心的问题，因为这是先行前提，也是问题或事物的根本。 当软件的规模越来越大，复杂度不断增加，软件项目开发维护过程中的问题就逐步暴露出来：软件产品质量低劣、软件维护工作量大、成本不断上升、进度不可控、程序人员无限度地增加。所以在60年代，“软件危机”的概念被提出来。 而软件工程提出来就是为了研究和克服“软件危机”的（1968年提出“软件工程”这一术语）！ 3. 我们会在哪里运用到软件工程？（Where?） 在考虑这个问题的时候，我们首先要思考一下，软件项目中间有几个阶段？项目中又有几个哪些角色参与其中？ 一般情况下，我们认定的软件项目生命周期为：需求定义与分析、设计、实现、测试、交付和维护 其次，软件项目参与角色包括：项目经理、产品经理、架构师、程序员、测试工程师、运维工程师 其实以上就是目前最为认可且常见的软件开发流程，绝大多数公司就是采用这种方式，进行软件开发与维护的，这不就是一种最为常见的软件工程的过程化方式吗？😎 4. 那我们应该怎么运用软件工程呢？（How？） 上文提到的软件项目生命周期，及参与角色衍生出了一套最为基础的过程模型：瀑布模型（如下图） 其后，当瀑布模型不能很好的应对需求的变更时，又衍生出了V模型、快速原型模型、增量模型、螺旋模型等等，试图改善瀑布模型存在的一些缺陷。这些改进模型的发展趋势上就是缩短项目周期，快速迭代。 这样到了90年代，各种轻量级开发方法例如Scrum、极限编程等也不断被提出。到了2001年，这些轻量级开发方法一起组成了敏捷联盟，其后敏捷开发如同星星之火，逐渐形成燎原之势。 此为，美国著名的软件工程专家巴利·玻姆 (Barry Boehm)于1983年提出了软件工程的七条基本原理。 用分阶段的生命周期计划严格管理 这一条是吸取前人的教训而提出来的。统计表明，50%以上的失败项目是由于计划不周而造成的。在软件开发与维护的漫长生命周期中，需要完成许多性质各异的工作。这条原理意味着，应该把软件生命周期分成若干阶段，并相应制定出切实可行的计划，然后严格按照计划对软件的开发和维护进行管理。 玻姆认为，在整个软件生命周期中应指定并严格执行6类计划：项目概要计划、里程碑计划、项目控制计划、产品控制计划、验证计划、运行维护计划。 坚持进行阶段评审 统计结果显示：大部分错误是在编码之前造成的，大约占63%错误发现的越晚，改正它要付出的代价就越大，要差2到3个数量级。 因此，软件的质量保证工作不能等到编码结束之后再进行，应坚持进行严格的阶段评审，以便尽早发现错误。 实行严格的产品控制 开发人员最痛恨的事情之一就是改动需求。但是实践告诉我们，需求的改动往往是不可避免的。这就要求我们要采用科学的产品控制技术来顺应这种要求。也就是要采用变动控制，又叫基准配置管理。当需求变动时，其它各个阶段的文档或代码随之相应变动，以保证软件的一致性。 采纳现代程序设计技术 从六、七十年代的结构化软件开发技术，到最近的面向对象技术，从第一、第二代语言，到第四代语言，人们已经充分认识到：方法大于气力。采用先进的技术既可以提高软件开发的效率，又可以减少软件维护的成本。（技术与时俱进） 结果应能清楚地审查 软件是一种看不见、摸不着的逻辑产品。软件开发小组的工作进展情况可见性差，难于评价和管理。为更好地进行管理，应根据软件开发的总目标及完成期限，尽量明确地规定开发小组的责任和产品标准，从而使所得到的标准能清楚地审查。 开发小组的人员应少而精 开发人员的素质和数量是影响软件质量和开发效率的重要因素，应该少而精。 这一条基于两点原因：高素质开发人员的效率比低素质开发人员的效率要高几倍到几十倍，开发工作中犯的错误也要少的多；当开发小组为N人时，可能的通信信道为N(N-1)/2, 可见随着人数N的增大，通讯开销将急剧增大。 承认不断改进软件工程实践的必要性 遵从上述六条基本原理，就能够较好地实现软件的工程化生产。但是，它们只是对现有的经验的总结和归纳，并不能保证赶上技术不断前进发展的步伐。因此，玻姆提出应把承认不断改进软件工程实践的必要性作为软件工程的第七条原理。根据这条原理，不仅要积极采纳新的软件开发技术，还要注意不断总结经验，收集进度和消耗等数据，进行出错类型和问题报告统计。这些数据既可以用来评估新的软件技术的效果，也可以用来指明必须着重注意的问题和应该优先进行研究的工具和技术。 5. 最后 近些年，云计算、微服务这些新技术的产生，也对软件工程产生了影响。云服务让分工更细，很多企业可以将运维、服务器维护、DBA、甚至某些独立服务交给云服务商；微服务让大团队变成小团队，每个小团队可以更专注于细分领域，减少相互之间的依赖。 当我们大致了解整个软件工程的演变发展史，会发现，软件工程的知识，都是建立在软件项目的过程，或者说软件项目生命周期之上的。 基于软件过程，我们有了角色分工，有了对过程的管理和工具，对过程中每个阶段细分的方法学和工具。 软件工程 = 过程 + 方法 + 工具 参考文章与文献： 梦见，《你真的理解 “软件工程” 吗？ 》 desaco，《浅谈对软件工程的认识与理解》 鱼天翱，《软件工程之怎么理解软件工程》 宝玉，《到底应该怎样理解软件工程？》 ","link":"https://hezhiming1995.github.io/post/ruan-jian-gong-cheng-de-ren-zhi-yu-li-jie/"},{"title":"Tableau学习笔记","content":" 一、为什么是Tableau？ 二、案例分析与基本图例 三、Tableau高级图例 四、拓展与课程总结 ——Tableau Software致力于帮助人们查看并理解数据。Tableau 帮助任何人快速分析、可视化并分享信息。超过 42,000 家客户通过使用 Tableau 在办公室或随时随地快速获得结果。数以万计的用户使用 Tableau Public 在博客与网站中分享数据。 一、为什么是Tableau？ 二、案例分析与基本图例 三、Tableau高级图例 四、拓展与课程总结 ","link":"https://hezhiming1995.github.io/post/tableau-xue-xi-bi-ji/"},{"title":"字节校招笔试(08-09) 算法题回溯与感想(下)","content":"第三道算法题：多米诺骨牌 题目： 小高最近迷上了玩多米诺骨牌：沿直线将一长串排摆放起来，推倒一个引起连锁反应，非常有趣。现在有 n 块牌，每张牌都有各自的高度和宽度(分别记为hih_ihi​和wiw_iwi​)。小高的摆放规则是，后面的牌的高度和宽度必须都大于前面的牌，请问小高用这n张牌最多能选出多少张组成一个最长牌阵呢？ 输入描述： 第一行 一个整数n 接下来n行，每行两个整数hih_ihi​和wiw_iwi​.（所有牌中没有长度相同或者宽度相同的两张牌） 输出描述： 一个数X，表示最多能选出X张组成最长牌列 样例输入： 5 5 5 3 1 2 6 4 2 1 4 样例输出： 3 （第一张牌：3 1，第二张牌：4 2，第三张牌：5 5） 思路：（动态规划） 请注意题目要求，题目要求的是长度和宽度都必须大于前面的牌，且所有的长宽都没有相同的！所以，我们只需要排序一组，比如排序长度，那么，我们在长度的排序列表中是不是找到一个最长的关于宽度的连续子序列，是不是就是最优解呢？ 这是一个最长上升子序列的经典问题，一般的解法有两种，一种是动态规划算法，复杂度是O(n2n^2n2)的；另外一种是基于二分查找，复杂度是O(nlognnlognnlogn)。 在这里，我们会将两种解法的代码都贴出来。（面鲸大大说这是一道送分题，回过头来看确实😂当时太菜了） 代码（动态规划）：By Python #!/usr/bin/env python3 # -*- coding: utf-8 -*- # 读取第一行输入 n =int(input()) # 读取第二行开始的n行输入 for i in n: h[i],w[i] = map(int,input().split(' ')) print(h,w) # 定义完美字符串的长度变量 maxLength = 0 # 遍历26个字母，并匹配完美字符串的字母 for i in range(26): # 定义左指针（完美字符串的起点）和容错（可替换的字母个数） left, faultTolerant = 0,0 # 定义一个右指针（完美字符串的终点），右指针大小从0到母字符串的长度 for right in range(n): # 当且仅当右指针指向的字母的ASCII码 不等于 第一个for循环指定的字母的ASCII码 if ord(s[right]) != ord('a') + i: # 因为右指针不等于当前for循环的26个字母中之一，所以容错加1 faultTolerant += 1 #如果容错大于了输入给定的最大容错m，这个时候没办法了，就只能动左指针了（右移） while faultTolerant &gt; m: # 存在着一种情况，左指针目前匹配的刚好是for循环指定的字母的ASCII码，此情况就不能减少容错 # 如果左指针当前匹配不是for指定的字母的ASCII码，那么右移左指针不就可以减少容错了吗？ if ord(s[left]) != ord('a') + i: faultTolerant -= 1 # 注意缩进，如果在if循环中，意味着只有当左指针不匹配ACSII码，左指针才+1，显然不符合逻辑 left += 1 # 注意缩进 maxLength = max(maxLength, right-left+1) print(maxLength) 第四道算法题 题目： 据说是最难的一道题，当然我没有写。不过不影响我去研究他，希望可以吃透！ 在n（1≤\\leq≤n$\\leq35）个正整数中，任意挑选k个（不可重复挑选，035）个正整数中，任意挑选k个（不可重复挑选，035）个正整数中，任意挑选k个（不可重复挑选，0\\leqkkk\\leq$n）数字的和记为sum，另有一个正整数m，请问sum%m的最大值是多少？ 输入描述： 第一行两个整数，分别是n和m 第二行为n个正整数 输出描述： 一个数x，x表示sum%m的最大值 样例输入： 5 5 1 2 3 4 5 样例输出： 4 思路 ","link":"https://hezhiming1995.github.io/post/zi-jie-xiao-zhao-bi-shi-08-09-suan-fa-ti-hui-su-yu-gan-xiang-xia/"},{"title":"字节校招笔试(08-09) 算法题回溯与感想(上)","content":"——记录一下四道字节算法题，并解决它们！谨以为耻！ 前言 字节跳动算法笔试(2020年8月9日)记录 我很惭愧，因为自己大概有两年多没有接触算法了，加上自己没有刷题，昨晚的字节跳动四道算法题，一题都没有得分，中间两道有思路，代码也写出来了。只是没有编译成功和得到预期的结果！ 其实最近几天疯狂想冲一个leetcode会员，本着都已经花了这么多钱，你不物尽其用，不心疼吗？但是还是忍了一手，因为毕竟这终究是人生一道门槛必须要跨过去的那一种！ 所以，就算是你不充钱，你也必须要去刷题，别！无！选！择！ 那就放手一搏吧！（一棵参天大树，最好的种植时间那就是：十年前和此刻！） 很多部分思路想法参考借鉴了微信博主面鲸，感谢您，附上您的微信公众号二维码: 第一道算法题：完美字符串 第一道算法题考试的时候想了几分钟，没思路，所以没有写，但是后来看牛客网的讨论区，有人说在leetcode上面刷到过原题，再次感慨，算法题，真的是必须要去刷题，怪自己前几个月毫无根据的自信儿！不过现在也不算太晚！ 题目： 长度为 n(1&lt;=n&lt;=500000) 的字符串，均为小写字母，允许修改m(1&lt;=m&lt;=n)个位置的字母，修改完毕后选取这个字符串的连续子串，满足这个子串只有一种字母，这个子串就是完美子串，求最长完美子串的长度。 样例输入： 8 1 aabaabaa 样例输出： 5 （把第3位或者第6位的任意一个b替换为a，得到aaaaa,该完美字串的长度为5） 思路：（双指针算法） 首先需要明确的是这个最完美的子串，无论长度多少，他都可能是由26个字母中的任意一个组成的。所以这里是不是需要在代码的最外面套一个for循环来一遍一遍的确定完美子串的组成字母呢？ 其次，假设我们找到了完美子串的组成字母是c，看一下题目要求，求得是完美子串的长度，所以我们是不是需要再来一个for循环从母字符串中的第一位来一遍一遍的c的连续性呢？ 第三点，这也是本题的最关键点！题目要求允许允许替换m个位子的字母，是不是表示字符串的最大容错为m。那么如何通过代码得出这一段最大容错为m的完美字符串呢？ 考试的时候就是卡在这里，结束后，参考学习了一下大佬们的思路，茅塞顿开！ 网上的解法是：这是典型的用two pointers来解决的问题，设置两个指针，left和right，分别考虑的是当前for循环所考虑的子串的起点和终点，每次先移动右指针，如果右指针指向的不是当前考虑的字母，那么用一个容错来改变他，直到容错大于了给定的最大容错，这个时候就需要动左指针了（右移），不停的移动，直到容错不大于最大容错时(等于的时候，就开始右指针的移动)，这个时候，left和right所指向的子串就是一个满足要求的潜在的一个可能的完美子串。 时间复杂度： O(n*26) 代码：By Python #!/usr/bin/env python3 # -*- coding: utf-8 -*- # 读取第一行输入 n,m = input().split(' ') n,m = int(n),int(m) # 读取第二行输入 s = input() # 定义完美字符串的长度变量 maxLength = 0 # 遍历26个字母，并匹配完美字符串的字母 for i in range(26): # 定义左指针（完美字符串的起点）和容错（可替换的字母个数） left, faultTolerant = 0,0 # 定义一个右指针（完美字符串的终点），右指针大小从0到母字符串的长度 for right in range(n): # 当且仅当右指针指向的字母的ASCII码 不等于 第一个for循环指定的字母的ASCII码 if ord(s[right]) != ord('a') + i: # 因为右指针不等于当前for循环的26个字母中之一，所以容错加1 faultTolerant += 1 #如果容错大于了输入给定的最大容错m，这个时候没办法了，就只能动左指针了（右移） while faultTolerant &gt; m: # 存在一种情况，左指针目前匹配的刚好是for循环指定字母的ASCII码，此情况就不能减少容错 # 如果左指针当前匹配不是for指定的字母的ASCII码，那么右移左指针不就可以减少容错了吗？ if ord(s[left]) != ord('a') + i: faultTolerant -= 1 # 注意缩进，如果在if循环中，则表示仅当左指针不匹配ACSII码，左指针才+1，显然不符合逻辑 left += 1 # 注意缩进 maxLength = max(maxLength, right-left+1) print(maxLength) 第二道算法题：交替序列和 这一道题相对而言，比较简单，因为可以套for循环，暴力求解出来，只是可惜我没有跑出正确答案，应该是自己中间代码出了一些逻辑上面的问题吧！ 题目： 给定一个长度为n的整数序列 A1A_1A1​, A2A_2A2​, ... , ANA_NAN​,找出其中的一段连续子序列，使得该段连续子序列的交替和最大。序列 ApA_pAp​, Ap+1A_{p+1}Ap+1​, ..., AqA_qAq​ 的交替和为：ApA_pAp​ - Ap+1A_{p+1}Ap+1​ + Ap+2A_{p+2}Ap+2​ - Ap+3A_{p+3}Ap+3​ + ... +(−1)q−p(-1)^{q-p}(−1)q−p *AqA_qAq​ 。 输入描述： 输入的第一行为一个正整数n(n&lt;=1e5)，表示序列的长度 输入的第二行为n个整数，依次给出了 A1A_1A1​, A2A_2A2​, ... , ANA_NAN​。(−105-10^5−105 ≤\\leq≤ AiA_iAi​ ≤\\leq≤ 10510^5105) 输出描述： 输出一个整数，表示最大的序列交替和 样例输入： 5 1 2 3 4 5 样例输出： 5 （就只有最后一个数字5，如果有序列的话，第二位永远比第一位数字大） 样例输入： 5 1 -2 3 -4 5 样例输出： 15 （ 1-(-2)+3-(-4)+5 = 15 ） 思路：（动态规划算法） 以下思路参考面鲸公众号的题目分析 做这个题之前我们先看看另外一个问题。给定一个长度为n的整数序列，找出其中一段连续子序列，使得该段连续子序列的和最大。这是一道经典的动态规划-「最大子段和问题」 对于最大子段和问题，假设CiC_iCi​是 A1A_1A1​, A2A_2A2​, ... , AiA_iAi​ 包含AiA_iAi​的向前连续延伸的最大子段之和。那么以结尾的，有两种情况： 第一种，这个最大子序列中只有AiA_iAi​时，那么CiC_iCi​=AiA_iAi​ 第二种，这个子序列包含了Ai−1A_{i-1}Ai−1​时，那么这个时候是不是可以想象成CiC_iCi​=Ci−1C_{i-1}Ci−1​+AiA_iAi​，有一种递归的思路 所以我们可以得到「状态转移方程」，CiC_iCi​=max (Ci−1C_{i-1}Ci−1​+AiA_iAi​, AiA_iAi​) 考虑到这种递归的想法的时候，就需要考虑到 「边界情况」，当 i=1时，我们同样需要考虑用A1A_1A1​和不用A1A_1A1​这两种情况 第一种，用 A1A_1A1​，那么 C1C_1C1​=A1A_1A1​ 第一种，不用 A1A_1A1​，那么 C1C_1C1​=0 因此，C1C_1C1​=max ( 0, A1A_1A1​) 到这里思路就特别的清晰了吧，对于对于本地，这个子段是连续的交替加减，那么我们是不是可以将这个给定的数组变成两个数组，对于某一个元素AiA_iAi​，一个数组中AiA_iAi​为正整数,另一个数组AiA_iAi​为负整数。 这里还有一点需要注意，我发现面鲸大大没有在文章中说出来！就是如果AiA_iAi​是数组中的第一位时，我们不能改变其正负性。所以变动只能从第二位开始，且如果第二位整数没有改变正负性，那么就设置第一位元素为0，以适配整体的算法逻辑。举例如下： 假设输入的数组为：[a, b, c, d] 将其变为两个数组，其中一个是：[a, -b, c, -d] 另一个是：[0, b, -c, d]，这样子就可以适配一个算法逻辑了 附上LeetCode上动态规划算法的逻辑指导图：太绝妙了这个算法！ 代码：By Python #!/usr/bin/env python3 # -*- coding: utf-8 -*- # 读取第一行输入，输入数组长度 n = int(input()) # 读取第二行输入，输入的数组 m0 = list(map(int,input().split(' '))) m1 = m0.copy() # 为了节约空间和时间成本这里用两个for循环来展现两个正负性不同的数组 # m0列表的偶数位元素变为负，m1列表的奇数位元素变为负 for i in range(1,n,2): m0[i],m1[i-1] = -m0[i],-m1[i-1] # 考虑到如果数组长度为奇数，则奇数位变负数组的最后一项在上述for循环中未变负 if len(m1)%2 == 1: m1[-1] = -m1[-1] # 因为题目规定了最大子序列的首项不改变正负性，所以设定m1数组的首项为0，以适应相同的规划算法 m1[0] = 0 # 动态规划算法！太绝妙了！ for i in range(1,n): m0[i] += max(m0[i-1],0) m1[i] += max(m1[i-1],0) print(max(max(m0),max(m1))) ","link":"https://hezhiming1995.github.io/post/字节校招笔试(08-09)-算法题回溯与感想(上)/"}]}